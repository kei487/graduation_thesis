\chapter{移動ロボットの経路計画問題}\label{chap:problem}

\section{問題設定}\label{sec:prob_def}
本研究では，移動ロボットを平面上で自律移動させる問題を扱う．
これは，ROS 2\cite{ROS2}で広く用いられているNavigation2\cite{Navigation2}
（ROS 2 の標準ナビゲーションパッケージ）が扱う問題と同様の問題である．
ロボットは，行動を始めるタイミングで目的地の座標を与えられ，
障害物との衝突を避けながらできる限り短い時間で目的地まで
到達しなければならない．
ロボットが移動を行う空間を環境と言い，
図\ref{fig:robot_env}にその環境の例を示す．

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{example-image-16x9.pdf}
  \caption{An enviroment for mobile robot navigation}
  \label{fig:robot_env}
\end{figure}

環境には，世界座標系が 2 次元の直交座標系で設定されており，
ロボットは，位置$(x, y)$と，$x$軸となす角$\theta$を向きとして持っている．
これらをまとめてロボットの状態（位置と向き）$s = (x， y， \theta)$
と，３つの変数で表す．

目的地（ゴール）は，$XY$平面上の座標あるいは，
$xy\theta$空間上の座標として与えられ，
図中の destination area のように
その座標を中心とした$XY$平面上の領域や，
$xy\theta$空間内の領域として扱われる．
実装によっては，任意の領域を目的地とすることができる．

環境中には，地図に記されており
その位置が既知である固定障害物と
地図に記されない未知障害物が存在する，
未知障害物のうち，特に
ロボットが移動する時間スケールで
位置が変わる人や他のロボットなどの移動障害物と呼ぶ．
未知障害物は，価値反復ROSパッケージの既存の機能で対処可能である．
しかし，本研究では，走り出しに関与する大域経路計画を扱うため，
局所経路計画である未知障害物の回避は陽には扱わない．

\subsection{マルコフ決定過程（Markov Decision Process: MDP）による定式化}

MDPは，エージェントが環境と相互作用しながら学習・行動決定を行うための数理モデルであり，
以下の4つの要素の組 
$\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R} \rangle$ で定義される．

\begin{enumerate}
    \item \textbf{状態空間 $\mathcal{S}$ (State Space)}:
    ロボットが取り得るすべての状態の集合．
	\ref{sec:prob_def}章で述べたように，価値反復ROSパッケージでは，
	各グリッドセル $(x, y)$ とロボットの方位$\theta$を合わせた
	$(x, y, \theta)$を一つの状態 $s \in \mathcal{S}$ に対応する．

    \item \textbf{行動の集合 $\mathcal{A}$ (Action Space)}:
    各状態でロボットが選択可能な行動の集合．
	価値反復ROSパッケージでは，速度$v$と角速度$\omega$の組で表される．
	例えば直進であれば，$(v, \omega) = (0.3[m/s], 0[rad/s]), (0.2, 0.1)$
	行動 $a \in \mathcal{A}$ となる．

    \item \textbf{状態遷移モデル $\mathcal{P}(s' | a, s)$ (Transition Probability)}:
    状態 $s$ で行動 $a$ を選択したときに，次の時刻に状態 $s'$ へ遷移する確率．
    \begin{equation}
        \mathcal{P}_a(s, s') = \Pr(S_{t+1}=s' \mid S_t=s, A_t=a)
    \end{equation}
    決定論的な環境（A*などが想定する世界）では，ある行動を行えば100\%意図した隣接セルへ移動する．
	しかし実環境では，移動はタイヤのスリップや慣性などといった要因による誤差が含む．
	MDPではこの不確実性を確率分布としてモデル化する．
	例えば，「前進」を選択しても，10\%の確率で左右のセルに移動し，
	5\%で行き過ぎて奥のセルに移動する，といった表現が可能である．

    \item \textbf{報酬モデル $\mathcal{R}(s, a, s')$ (Reward Function)}:
    状態遷移に伴って得られる評価でスカラで表される．
	経路計画問題においては，コストの最小化または負の報酬の最大化として定式化される．
    	例えば，ゴール状態に到達したときに大きな正の報酬を与え，
	障害物に衝突したときに大きな負の報酬（ペナルティ）を与える．
	また，移動にかかる時間やエネルギーを表現するため，
	各ステップごとにわずかな負の報酬（ステップコスト）を与える．
\end{enumerate}

\subsubsection{ベルマン方程式と価値関数}
MDPの目的は，各状態でどのような行動をとるべきかというルール，
すなわち方策（Policy） $\pi: \mathcal{S} \to \mathcal{A}$を見つけることである．
最適な方策 $\pi^*$ を見つけるために，状態価値関数 $V^\pi(s)$を導入する．
状態価値関数は式（\ref{eq:value_func_def}）のように定義される．
これは，ある状態 $s$ からスタートし，方策 $\pi$ に従って行動し続けたときに
得られる将来の報酬の総和（期待値）である．

\begin{equation}
    V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, \pi \right]
  \label{eq:value_func_def}
\end{equation}

最適な方策 $\pi^*$ に従ったときの価値関数を最適状態価値関数 $V^*(s)$ と呼ぶ．
$V^*(s)$ は，式（\ref{eq:bellman_eq}）に示すベルマン方程式（Bellman Equation）
を満たす．
ベルマン方程式は再帰的な構造をしており，
ある状態の価値は，そこで最適な行動をとった際に期待される即時報酬と，
遷移先の状態の価値の和によって決まることを意味している．
\begin{equation}
    V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} 
	\mathcal{P}(s'| a, s) \left[ \mathcal{R}(s, a, s') + V^*(s') \right]
    \label{eq:bellman_eq}
\end{equation}




\section{価値反復アルゴリズム}
VIパッケージの価値反復で計算される式は，
ロボットがある位置・向き
$\boldsymbol{x}$にあるとき，
そこから目的地までのコストを計算した
状態価値関数
\begin{align}
	V: \mathcal{S} \rightarrow \mathbb{R}
\end{align}
である．ここで$\mathcal{S}$は，
$XY\theta$空間を格子状に離散化した
離散状態$s$の集合である．
また，コストというのは，時間と，
時間換算で与えるペナルティーを足した値である．
ペナルティーは悪路に相当する$s$など，ロボットが
入るとなんらかの問題のある状態に与えられる．

$V$は，価値反復の計算が進むと正解の値に近づいていく．
このとき，$V$の値は図\ref{fig:propose}(b)のように
目的地の周りから収束していき，
目的地から遠いところが最後に収束する．
この計算は探索ではないが，
「幅優先探索」に相当する処理となる．


$V$が収束すると，ロボットは$V$の値が小さくなるような行動を
選択し続けることで，最適な経路を選択して移動できるようになる．
また，$V$が完全に収束しなくても，
ロボットのいる$s$から目的地に向かって値が
単調減少していると，ロボットは目的地に向かうことができる．
このような状態を，本稿では「経路が見つかる」と表現する．

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{example-image-16x9.pdf}
  \caption{Convergence of the value function}
  \label{fig:vi_convergence}
\end{figure}


1章で述べたとおり，
価値反復は状態価値関数$V$を計算する．
VIパッケージの実装では，
$xy\theta$空間を格子状に離散化して
作った離散状態の集合$\mathcal{S}$の要素$s$
に対し，ひとつひとつコスト$V(s)$が計算される．
%つまり，
%$V: \mathcal{S} \rightarrow \mathbb{R}$
%という形式をとる．


$V(s)$には初期値として，$s$が目的地の領域
に含まれる場合に$0$，
そうでないときは無限大（実装上は目的地まで10万秒など，
非常に大きな値）が与えられる．
価値反復は終端状態以外の$V(s)$の値を，
繰り返し計算で実際のコストまで値を小さくしていく．


完全に収束した$V$は最適状態価値関数$V^*$と呼ばれ，
$V^*$からは，最適な行動を求めることができる．
ただし，ロボットが移動を開始するためには，$V^*$
を厳密に計算する必要はない．
現在地の周辺において，目的地に向かうための適切な
勾配が$V$にできると，
ロボットは目的地に向かう（$V$のコストを減らす）
ように行動をとることができる．

しかしながら，この勾配の伝播は
A*などの探索手法が経路を探すよりも遅い．
価値反復は，探索の手法として見ると幅優先探索になっており，
$V$の勾配は目的地の周辺からできはじめ，
それがより遠い地点に伝播していく．
またこの伝搬の計算は，ロボットが通らないであろう
経路でも行われる．
そのため，目的地までの経路が存在すれば，
それが複雑に入り組んでいても
必ず見つけることはできるが，
ロボットは現在地に勾配が到達するまで，
長く待たされることになる．
価値反復アルゴリズムは，ベルマン方程式の形式に乗るように
定式化することで，最適な状態価値関数を得られるアルゴリズムである．



\subsection{価値反復アルゴリズム}
ベルマン最適方程式 (\ref{eq:bellman_opt}) を用いて，
反復計算により $V^*(s)$ を求める手法が価値反復法（Value Iteration）である．
アルゴリズムの手順は以下の通りである．

\begin{enumerate}
    \item \textbf{初期化}: すべての状態 $s$ について，$V(s)$ を任意の値（通常は0）に初期化する．
    \item \textbf{反復更新}: 以下の更新式を，すべての状態 $s$ に対して適用する．
    \begin{equation}
        V_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} 
	\mathcal{P}_a(s, s') \left[ \mathcal{R}_a(s, s') + \gamma V_k(s') \right]
    \end{equation}
    ここで $k$ は反復回数を表す．
    \item \textbf{収束判定}: 全状態における価値関数の更新量 $|V_{k+1}(s) - V_k(s)|$ の最大値が，
	事前に定めた閾値 $\epsilon$ 未満になれば停止する．
    \item \textbf{方策抽出}: 収束した価値関数 $V^*(s)$ を用い，
	各状態で価値を最大化する行動を選択する貪欲方策（Greedy Policy）を得る．
    \begin{equation}
        \pi^*(s) = \operatorname*{argmax}_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} 
	\mathcal{P}_a(s, s') \left[ \mathcal{R}_a(s, s') + \gamma V^*(s') \right]
    \end{equation}
\end{enumerate}

この計算により，環境内のあらゆる場所からゴールへ向かうための最適な「勾配」が得られる．
これはポテンシャル場に似ているが，ポテンシャル法が抱える「局所解（Local Minima）」の問題
（ゴール以外の窪みにハマって出られなくなる現象）が発生しないという強力な数学的保証がある．
なぜなら，ベルマン方程式による更新は，大域的な最適性を伝播させる処理だからである．

% \section{経路探索アルゴリズム}
% 
% \subsection{Dijsktra法}
% よく知られたグラフ探索アルゴリズムである．
% 
% \subsection{A*アルゴリズム}
% Dijkstra法を発展させたものであり，ゴールまでのコストを推定する
% ヒューリスティック関数を追加したものである．
